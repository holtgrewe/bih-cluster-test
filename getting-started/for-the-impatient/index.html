<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="BIH HPC IT">
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>For the Impatient - BIH HPC Docs</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../css/theme.css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "For the Impatient";
    var mkdocs_page_input_path = "getting-started/for-the-impatient.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> BIH HPC Docs</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../..">BIH HPC Documentation</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Admin</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../admin/maintenance-windows/">Maintenance windows</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Getting started</span></p>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">For the Impatient</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#cluster-hardware-and-scheduling">Cluster Hardware and Scheduling</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#differences-between-workstations-and-clusters">Differences Between Workstations and Clusters</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-the-cluster-is-and-is-not">What the Cluster Is and Is NOT</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#locations-on-the-cluster">Locations on the Cluster</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#temporary-directories">Temporary Directories</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#first-steps-on-the-cluster">First Steps on the Cluster</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#connecting-to-the-cluster">Connecting to the Cluster</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#connecting-to-compute-node-through-login-node">Connecting to Compute Node through Login Node</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#submitting-jobs">Submitting Jobs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#inspecting-jobs-and-the-cluster">Inspecting Jobs and the Cluster</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Miscellaneous</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../miscellaneous/publication-list/">Publication List</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">BIH HPC Docs</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Getting started &raquo;</li>
        
      
    
    <li>For the Impatient</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/holtgrewe/bih-cluster-test/edit/master/bih-cluster/docs/getting-started/for-the-impatient.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="for-the-impatient">For the Impatient</h1>
<p>This document describes the fundamentals of using the BIH cluster.
More detailed documentation is available in the rest of the Wiki.
Start exploring from the <a href="home">Wiki home</a>.</p>
<h2 id="cluster-hardware-and-scheduling">Cluster Hardware and Scheduling</h2>
<p>The cluster consists of the following major components:</p>
<ul>
<li>2 login nodes for users <code>med-login1</code> and <code>med-login2</code> (for interactive sessions only),</li>
<li>2 nodes for file transfers <code>med-transfer1</code> and <code>med-transfer2</code>,</li>
<li>a scheduling system using SLURM (SGE is being phased out),</li>
<li>approximately 200 general purpose compute nodes <code>med01XX</code>, <code>med02XX</code>, <code>med05XX</code>, <code>med06XX</code>, <code>med07XX</code>.</li>
<li>a few high memory nodes <code>med040[1-4]</code>,</li>
<li>4 nodes with 4 Tesla GPUs each (!) <code>med030[1-4]</code>,</li>
<li>a high-performance, parallel GPFS file system with 2.1 PB, by DDN mounted at <code>/fast</code>,</li>
<li>a slower "classic" ZFS file system available through NFS with ~250 TB mounted at <code>/slow</code>.</li>
</ul>
<p>This is shown by the following picture:</p>
<p><img alt="" src="figures/Cluster_Layout.png" /></p>
<h2 id="differences-between-workstations-and-clusters">Differences Between Workstations and Clusters</h2>
<p>The differences include:</p>
<ul>
<li>The directly reachable login nodes are not meant for computation!
  Use <code>sqrun</code> to go to a compute node.</li>
<li>Every time you type <code>srun</code> to go to a compute node you might end up on a different host.</li>
<li>Most directories on the nodes are not shared, including <code>/tmp</code>.</li>
<li>The <code>/fast</code> directory is shared throughout the cluster which contains your home, group home, and project directories.</li>
<li>You will not get <code>root</code> or <code>sudo</code> permissions on the cluster.</li>
<li>You should use <em>batch jobs</em> (<code>sbatch</code>) over calling programs interactively.</li>
</ul>
<h2 id="what-the-cluster-is-and-is-not">What the Cluster Is and Is NOT</h2>
<p>NB: the following might sound a bit harsh but is written with everyone's best intentions in mind (we actually like you, our user!)
This addresses a lot of suboptimal (yet not dangerous, of course) points we observed in our users.</p>
<p><strong>IT IS</strong></p>
<ul>
<li>It is scientific infrastructure just like a lab workbench or miscroscope.
  It is there to be used for you and your science.
  <strong>We trust you</strong> to behave in a collaboratively.
  We will monitor usage, though, and call out offenders.</li>
<li>With its ~200 nodes, ~6400 threads and fast parallel I/O, it is a powerful resource optimized for bioinformatics sequencing data analysis.</li>
<li>A place for data move data at the beginning of your project.
  By definition, every project has an end.
  Being the place it is, your project data needs to leave the cluster at the end of the cluster.</li>
<li>A collaborative resource with central administration managed by BIH HPC IT and supported via hpc-helpdesk@bihealth.de</li>
</ul>
<p><strong>IT IS NOT</strong></p>
<ul>
<li>A self-administrated workstation or servers.<ul>
<li>You will not get <code>sudo</code>.</li>
<li>We will not install software beyond those in broad use and available in CentOS Core or EPEL repositories.</li>
<li>You can install software in your user/group/project directories, for example using Conda.</li>
</ul>
</li>
<li>A place to store primary copies of your data.
  You only get 1 GB of storage in your home for scripts, configuration, and documents.</li>
<li>A safe place to store data.
  Only your 1 GB of home is in snapshots and backup.
  While data is stored on redundant disks, technical or administrative failure might eventually lead to data loss.
  We do everything humanly possible to prevent this.
  Despite this, it is your responsibility to keep important files in the snapshot/backup protected home, ideally even in copy (e.g., a git repository) elsewhere.
  Also, keeping safe copies of primary data files, your published results, and the steps in between reproducible is your responsibility.</li>
<li>A place to store data indefinitely.
  The fast GPFS storage is expensive and "sparse" in a way.
  The general workflow is: (1) copy data to cluster, (2) process it, creating intermediate and final results, (3) copy data elsewhere and remove it from the cluster</li>
<li>Generally suitable for primary software development.
  The I/O system might get overloaded and saving scripts might take some time.
  We know of people who do this and it works for them.
  Your mileage might vary.</li>
</ul>
<h2 id="locations-on-the-cluster">Locations on the Cluster</h2>
<ul>
<li>Your home directory is located in <code>/fast/users/$USER</code>.
  <strong>Your home is for scripts, source code, and configuration only.</strong>
  <strong>Use your <code>work</code> directory for large files.</strong>
  <strong>The quota in the <code>home</code> directory is 1 GB but we have nightly snapshots and backups thereof.</strong></li>
<li>Your work directory is located in <code>/fast/users/$USER/work</code>.
  This is where you should place large files.
  Files in this location do not have snapshots or backups.</li>
<li>The directory (actually a GPFS file set) <code>/fast/users/$USER/scratch</code> should be used for temporary data.
  <strong>All data placed there will be removed after 4 weeks.</strong></li>
<li>If you are part of an AG/lab working on the cluster, the group directory is in <code>/fast/groups/$AG</code>.</li>
<li>Projects are located in <code>/fast/projects/$PROJECT</code>.</li>
</ul>
<h3 id="temporary-directories">Temporary Directories</h3>
<p>Note that you also have access to <code>/tmp</code> on the individual nodes but the disk is <strong>slow</strong> and <strong>small</strong>.
If you are processing large NGS data, we recommend you create <code>/fast/users/$USER/scratch/tmp</code> and set the environment variable <code>TMPDIR</code> to point there.
However, for creating locks special Unix files such as sockets or fifos, <code>/tmp</code> is the right place.
<strong>Note that files placed in your <code>scratch</code> directory will be removed automatically after 4 weeks.</strong>
<strong>Do not place any valueable files in there.</strong></p>
<h2 id="first-steps-on-the-cluster">First Steps on the Cluster</h2>
<h3 id="connecting-to-the-cluster">Connecting to the Cluster</h3>
<ul>
<li>From the Charite, MDC, and BIH networks, you can connect to the cluster login nodes <code>med-login{1,2}.bihealth.org</code>.<ul>
<li>For Charite users, your name is <code>${USER}_c</code>, for MDC users, your account is <code>${USER}_m</code> where <code>$USER</code> is the login name of your primary location.</li>
</ul>
</li>
<li>From the outside, <strong>for MDC users</strong>, the cluster is accessible via <code>ssh1.mdc-berlin.de</code> (you need to enable SSH key agent forwarding for this)<ul>
<li>Note that you have to use your MDC user name (without any suffix <code>_m</code>) for connecting to this host.</li>
<li>Also note that BIH HPC IT does not have control over <code>ssh1.mdc-berlin.de</code>.
  <em>You have to contact MDC IT in case of any issues.</em></li>
</ul>
</li>
<li>From the outside, <strong>for Charite</strong> users, there is no SSH hop node.
  Instead, you have to apply for VPN through Charite Gesch√§ftsbereich IT.
  You can use <a href="https://intranet.charite.de/fileadmin/user_upload/portal/service/service_06_geschaeftsbereiche/service_06_14_it/VPN-Zusatzantrag_O.pdf">this form availble in Charite Intranet</a> for this.
  Please refer to the Charite intranet or helpdesk@charite.de for more information.</li>
</ul>
<h3 id="connecting-to-compute-node-through-login-node">Connecting to Compute Node through Login Node</h3>
<p>After logging into the cluster, you are on the login node <code>med-login&lt;X&gt;</code> (<code>&lt;X&gt;</code> can be either <code>1</code> or <code>2</code>).
When transferring files, use the <code>med-transfer1</code> or <code>med-transfer2</code> nodes.
You should not do computation or other work on the login or file transfer nodes, but use the compute nodes instead.
Typically, you'll create an interactive session on a compute node using the <code>srun</code> command.</p>
<h3 id="submitting-jobs">Submitting Jobs</h3>
<p>While not recommended, you can perform computations (such as using BWA) in the interactive session.
However, when the connection is interrupted, your computation process will be stopped.
It is therefore recommended you submit jobs using the <code>sbatch</code> command (or <a href="Manual-Useful-Tips-Working-with-Screen">use screen</a>).</p>
<h3 id="inspecting-jobs-and-the-cluster">Inspecting Jobs and the Cluster</h3>
<p>You can inspect your currently running jobs with <code>squeue</code>, and kill them using <code>scancel</code>.
You can inspect jobs that have finished with <code>sacct</code>, and see the cluster nodes using <code>sinfo</code>.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../miscellaneous/publication-list/" class="btn btn-neutral float-right" title="Publication List">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../admin/maintenance-windows/" class="btn btn-neutral" title="Maintenance windows"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>2016-2020, BIH HPC IT</p>
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/holtgrewe/bih-cluster-test/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../../admin/maintenance-windows/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../miscellaneous/publication-list/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
